{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "96a2d381-e5f8-47b9-977e-1c610644b72d",
    "_uuid": "4b799943-1fcf-496f-9383-81a596b9ad6b"
   },
   "source": [
    "These text snippets are randomly sampled from a raw dataset. Each sentence pair may or may not be semantically related. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "974a6fa3-259a-44b6-b0dc-0d646a254519",
    "_uuid": "b27b3a6b-98fd-41d1-97f2-7a937af2cf47",
    "execution": {
     "iopub.execute_input": "2021-05-28T19:49:40.890906Z",
     "iopub.status.busy": "2021-05-28T19:49:40.890543Z",
     "iopub.status.idle": "2021-05-28T19:49:42.619573Z",
     "shell.execute_reply": "2021-05-28T19:49:42.618773Z",
     "shell.execute_reply.started": "2021-05-28T19:49:40.890856Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#'nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')')\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "98523705-9fab-4229-9475-913c2e8ad5e5",
    "_uuid": "5270f56c-09bd-4e68-a64e-6a2699c1b2ee",
    "execution": {
     "iopub.execute_input": "2021-05-28T19:49:42.621470Z",
     "iopub.status.busy": "2021-05-28T19:49:42.621172Z",
     "iopub.status.idle": "2021-05-28T19:49:43.118046Z",
     "shell.execute_reply": "2021-05-28T19:49:43.117042Z",
     "shell.execute_reply.started": "2021-05-28T19:49:42.621433Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4023, 3)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../input/textsimilarity/Text_Similarity_Dataset.csv')\n",
    "print(data.shape)\n",
    "\n",
    "X = data.iloc[:,1:].values\n",
    "s1 = X[:,0]\n",
    "s2 = X[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1e25169b-fd0a-4f3c-a2db-6c4730457f45",
    "_uuid": "135ad458-87a7-47ef-89fc-a76d292f244e"
   },
   "source": [
    "### Preprocessing of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "7642b375-e362-4beb-9e65-53b81e7613c0",
    "_uuid": "dfab2553-110b-47bf-a4f0-9e1ff15c2eda",
    "execution": {
     "iopub.execute_input": "2021-05-28T19:49:43.119547Z",
     "iopub.status.busy": "2021-05-28T19:49:43.119237Z",
     "iopub.status.idle": "2021-05-28T19:49:43.129006Z",
     "shell.execute_reply": "2021-05-28T19:49:43.128112Z",
     "shell.execute_reply.started": "2021-05-28T19:49:43.119510Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer() #Lemmatization usually refers to doing things properly with the use of a vocabulary \n",
    "#and morphological analysis of words, normally aiming to remove inflectional endings only and to return the \n",
    "#base or dictionary form of a word. (lemma)\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "d5c25fb5-ebbe-4f4d-9a91-b8c125fe2f6c",
    "_uuid": "8c9e03c5-3494-4a3a-9fc6-3d41ba2d8cea",
    "execution": {
     "iopub.execute_input": "2021-05-28T19:49:43.130628Z",
     "iopub.status.busy": "2021-05-28T19:49:43.130302Z",
     "iopub.status.idle": "2021-05-28T19:49:43.141758Z",
     "shell.execute_reply": "2021-05-28T19:49:43.140845Z",
     "shell.execute_reply.started": "2021-05-28T19:49:43.130581Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "\n",
    "    tokens = [word_tokenize(str(sentence)) for sentence in s] #Splitting strings into tokens (nominally words). \n",
    "    #It splits tokens based on white space and punctuation. \n",
    "    #For example, commas and periods are taken as separate tokens. \n",
    "    #Contractions are split apart\n",
    "\n",
    "    rm = []\n",
    "    for w in tokens:\n",
    "        sm = re.sub('[^A-Za-z]',' ', str(w)) #removing non-alphabetical characters. Failed case -> one string has \"2\", another has \"two\".\n",
    "        x = re.split(\"\\s\", sm) \n",
    "        rm.append(x)\n",
    "\n",
    "    for sent in rm:\n",
    "        while \"\" in sent:\n",
    "            sent.remove('') #removing the empty elems\n",
    "\n",
    "    low = []\n",
    "    for i in rm:\n",
    "        i = [x.lower() for x in i] #converts all the chars to lowercase.\n",
    "        low.append(i)\n",
    "    \n",
    "    lemmatized = []\n",
    "    for sent in low:\n",
    "        tok = [wnl.lemmatize(w) for w in sent]\n",
    "        lemmatized.append(tok)\n",
    "    \n",
    "    filtered_sent = []\n",
    "    for sent in lemmatized:\n",
    "        toks = [w for w in sent if w not in stop_words]\n",
    "        filtered_sent.append(toks)\n",
    "        \n",
    "    return filtered_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "f7e8ef6b-6166-481f-8a9d-0f1af68e8093",
    "_uuid": "d777b483-f226-4360-8ae7-92b32fdbf24a",
    "execution": {
     "iopub.execute_input": "2021-05-28T19:49:43.144504Z",
     "iopub.status.busy": "2021-05-28T19:49:43.144181Z",
     "iopub.status.idle": "2021-05-28T19:52:02.663370Z",
     "shell.execute_reply": "2021-05-28T19:52:02.661726Z",
     "shell.execute_reply.started": "2021-05-28T19:49:43.144452Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "filter_words1 = preprocess(s1)\n",
    "filter_words2 = preprocess(s2)\n",
    "# 139.517s on kaggle kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dc58e97e-0996-4f92-bb25-53067ad3c30a",
    "_uuid": "90e55379-66bd-4f47-9afb-34fdb1cdbe21"
   },
   "source": [
    "### FastText\n",
    "FastText is an extension to Word2Vec proposed by Facebook in 2016. FastText divides words into several n-grams instead of feeding individual words into the Neural Network. For example, app, ppl, and ple are trigrams for the word apple. The sum of these n-grams will form the word embedding vector for apple. We will have word embeddings for all of the n-grams given the training dataset after training the Neural Network. Because some of their n-grams are likely to appear in other words, rare words can now be properly represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "5ec067e3-7f81-4ced-bfcf-2a4d981f48f8",
    "_uuid": "9f031363-d5fb-4ebe-aa8a-2169a28fa0a5",
    "execution": {
     "iopub.execute_input": "2021-05-28T19:59:09.010596Z",
     "iopub.status.busy": "2021-05-28T19:59:09.010076Z",
     "iopub.status.idle": "2021-05-28T19:59:27.149094Z",
     "shell.execute_reply": "2021-05-28T19:59:27.147705Z",
     "shell.execute_reply.started": "2021-05-28T19:59:09.010535Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.FastTextKeyedVectors'>\n",
      "[0.3194794, 0.32284385]\n"
     ]
    }
   ],
   "source": [
    "model1 = FastText('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz')\n",
    "model2 = FastText('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz')\n",
    "word_vector1 = model1.wv\n",
    "word_vector2 = model2.wv\n",
    "print(type(word_vector1))\n",
    "#vocabulary = word_vectors.vocab.items() -> obsolete in gensim 4x\n",
    "similarity = [word_vector1.similarity('woman', 'man'), word_vector2.similarity('woman','man')]\n",
    "print(similarity)\n",
    "# [0.3194794, 0.32284385] -> dissimilar value of similarity for different word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-28T20:01:17.399805Z",
     "iopub.status.busy": "2021-05-28T20:01:17.399417Z",
     "iopub.status.idle": "2021-05-28T20:01:17.407593Z",
     "shell.execute_reply": "2021-05-28T20:01:17.406605Z",
     "shell.execute_reply.started": "2021-05-28T20:01:17.399739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.8601252e-03  3.6149721e-03 -1.2410122e-03  3.5201798e-03\n",
      "  1.9627514e-03  1.5549501e-04 -3.0650198e-04 -2.4973419e-03\n",
      "  1.0811526e-03  1.0770498e-03 -1.7207910e-03  3.5857665e-04\n",
      " -1.6502569e-03  1.7301711e-03  3.3062345e-03  1.2433920e-03\n",
      " -7.1278092e-04 -3.2974439e-04 -8.4022281e-04  2.8064770e-03\n",
      " -1.5423523e-03 -2.3654152e-03 -9.9924277e-04  2.6055963e-03\n",
      "  1.6111322e-04  1.2526757e-03 -8.9503551e-04  7.4827758e-04\n",
      " -6.8006373e-04  1.4815993e-03  1.0589353e-03 -9.3595829e-04\n",
      "  1.3139204e-03  1.0518164e-03  6.4095814e-04  1.5005955e-03\n",
      "  1.1427484e-03 -1.4020366e-03 -8.3798834e-04  5.7458162e-04\n",
      "  5.3651596e-04  1.8680288e-03  1.4767486e-04 -3.2695434e-03\n",
      " -2.6735817e-03 -1.6171483e-03  3.7804416e-03 -6.1163031e-05\n",
      "  1.5303438e-03  2.2725803e-03  7.6203687e-05  2.5516321e-04\n",
      " -9.6527458e-04  3.5743082e-03 -3.1296138e-03  3.4323537e-03\n",
      " -1.5139274e-03 -1.4549088e-03  2.9783570e-03 -1.3296019e-03\n",
      " -1.0197319e-03  3.0602850e-03 -1.8883805e-03 -2.1588353e-03\n",
      " -8.6682744e-04  1.0607074e-03  3.4123191e-03  2.0999588e-04\n",
      " -1.9369498e-03  3.1413320e-03 -1.6553564e-03 -3.3722019e-03\n",
      " -7.1930757e-04 -5.7701254e-05 -7.9402921e-04 -4.9170008e-04\n",
      "  1.6104151e-03  1.5809658e-03  3.0622093e-04 -9.3963422e-04\n",
      " -3.7924465e-04 -1.5143090e-03 -1.0952599e-03 -2.1446678e-03\n",
      "  7.9981022e-04 -1.3402154e-03 -2.4162845e-03  9.7490411e-04\n",
      "  7.8450656e-04  1.4496975e-03 -2.0469408e-03 -8.1256311e-04\n",
      "  3.5832298e-03  1.2953377e-04 -2.0117157e-03  6.0476456e-04\n",
      "  4.5617617e-04  2.4611286e-03 -1.3970669e-03  1.2873393e-03]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(word_vector1['media'])\n",
    "print(len(word_vector1['saw'])) #100-dimension vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b2c20256-371d-48ed-b0c5-40abfc3255e0",
    "_uuid": "fbdce35b-fa4f-466a-a595-886b941609c3"
   },
   "source": [
    "## Cosine Similarity\n",
    "*Cosine similarity* calculates the similarity of two vectors by taking the cosine of the angle formed by the two vectors in their dot product space. If the angle is zero, their similarity is one; and as the angle goes up, the similarity goes down. Because the measure is independent of vector length, it is a popular measure for high-dimensional spaces.\n",
    "\n",
    "Other measures include *Jacard Similarity* and *Word mover distance* (independent of the words used in the dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "25936d6b-d9b8-4c89-9011-5598d13c1abf",
    "_uuid": "4b6196b3-4a55-4d8b-8632-0af6abd6d456",
    "execution": {
     "iopub.execute_input": "2021-05-28T19:52:19.449125Z",
     "iopub.status.busy": "2021-05-28T19:52:19.448474Z",
     "iopub.status.idle": "2021-05-28T19:55:09.266421Z",
     "shell.execute_reply": "2021-05-28T19:55:09.265529Z",
     "shell.execute_reply.started": "2021-05-28T19:52:19.449065Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result1, result2 = [], []\n",
    "\n",
    "for sent1, sent2 in zip(filter_words1,filter_words2):\n",
    "    vector11 = np.mean([word_vector1[word] for word in sent1], axis = 0)\n",
    "    vector12 = np.mean([word_vector1[word] for word in sent2], axis = 0)\n",
    "    vector21 = np.mean([word_vector2[word] for word in sent1], axis = 0)\n",
    "    vector22 = np.mean([word_vector2[word] for word in sent2], axis = 0)\n",
    "    cosine1 = scipy.spatial.distance.cosine(vector11, vector12)\n",
    "    cosine2 = scipy.spatial.distance.cosine(vector21, vector22)\n",
    "    result1.append((1-cosine1))\n",
    "    result2.append((1-cosine2))\n",
    "    \n",
    "data['Result 1'], data['Result 2'] = result1, result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "c443a99a-9d5d-4d90-bd1c-7353ecc7e059",
    "_uuid": "494f2bad-8b2e-47ce-b79c-8b673aa712f8",
    "execution": {
     "iopub.execute_input": "2021-05-28T19:55:09.268073Z",
     "iopub.status.busy": "2021-05-28T19:55:09.267727Z",
     "iopub.status.idle": "2021-05-28T19:55:09.530388Z",
     "shell.execute_reply": "2021-05-28T19:55:09.529454Z",
     "shell.execute_reply.started": "2021-05-28T19:55:09.268023Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unique_ID  Result 1  Result 2\n",
      "0           0  0.315760  0.418325\n",
      "1           1  0.385438  0.354219\n",
      "2           2  0.451542  0.454217\n",
      "3           3  0.436756  0.479157\n",
      "4           4  0.528687  0.514801\n",
      "5           5  0.379276  0.512622\n",
      "6           6  0.232499  0.261513\n",
      "7           7  0.510037  0.460600\n",
      "8           8  0.539121  0.498320\n",
      "9           9  0.396184  0.256629\n",
      "10         10  0.408196  0.441658\n",
      "11         11  0.252241  0.214241\n"
     ]
    }
   ],
   "source": [
    "ans  = data[['Unique_ID', 'Result 1', 'Result 2']]\n",
    "print(ans[:12])\n",
    "ans.to_csv('file1.csv', index=0)\n",
    "# Unique_ID  Result 1  Result 2  -> dissimlar values for different word vectors.\n",
    "#0           0  0.315760  0.418325\n",
    "#1           1  0.385438  0.354219\n",
    "#2           2  0.451542  0.454217\n",
    "#3           3  0.436756  0.479157\n",
    "#4           4  0.528687  0.514801\n",
    "#5           5  0.379276  0.512622\n",
    "#6           6  0.232499  0.261513\n",
    "#7           7  0.510037  0.460600\n",
    "#8           8  0.539121  0.498320\n",
    "#9           9  0.396184  0.256629\n",
    "#10         10  0.408196  0.441658\n",
    "#11         11  0.252241  0.214241"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
